{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from qwen_agent.agents import Assistant\n",
    "from qwen_agent.utils.output_beautify import typewriter_print, multimodal_typewriter_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_elements_with_vlm(image_path, mineru_layout, user_query, agent_instance):\n",
    "    \"\"\"\n",
    "    使用VLM提取页面中与用户查询相关的元素，并修正MinerU的布局噪声。\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): 本地图片路径或URL。\n",
    "        mineru_layout (list): MinerU生成的布局列表 (List of dicts)。\n",
    "        user_query (str): 用户想要查询的内容。\n",
    "        agent_instance (Assistant): 初始化的Qwen-Agent实例。\n",
    "        \n",
    "    Returns:\n",
    "        list: 包含相关元素的列表，格式与MinerU一致但经过清洗。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 将MinerU布局数据转换为字符串，作为上下文提供给模型\n",
    "    # 为了节省token，可以只保留 content 和 bbox，去掉 angle 等非必要字段\n",
    "    simplified_layout = []\n",
    "    for item in mineru_layout:\n",
    "        simplified_layout.append({\n",
    "            \"type\": item.get(\"type\"),\n",
    "            # Ensure bbox coordinates are integers\n",
    "            \"bbox\": [int(cord*1000) for cord in item.get(\"bbox\")],\n",
    "            \"angle\": item.get(\"angle\", 0),\n",
    "            \"content\": item.get(\"content\", \"\") # 截断过长文本以节省上下文\n",
    "        })\n",
    "    layout_context = json.dumps(simplified_layout, ensure_ascii=False)\n",
    "\n",
    "    # 2. 构建提示词 (Prompt)\n",
    "    # prompt_text = f\"\"\"\n",
    "    # 输入包含：\n",
    "    # 1. 视觉文档的页面图像。\n",
    "    # 2. 无查询感知的布局检测结果（MinerU格式），这仅作为参考，可能包含错误的粒度划分或不准确的边界框等噪声。\n",
    "    # 3. 用户的查询意图。\n",
    "    \n",
    "    # 请执行以下步骤：\n",
    "    # 1. 语义匹配：仔细观察图像，判断页面内容是否包含用户查询相关的证据信息。如果不相关，返回空列表。\n",
    "    # 2. 精准定位：如果相关，请提取与查询匹配的完整视觉证据链（可能包含多个文本块、表格、图像区域等元素）。\n",
    "    # 3. BBox修正：如果MinerU的 bbox 范围过大（包含查询无关内容）、过小（截断内容、过度拆分）或位置偏移，请根据图像实际视觉内容，生成新的、更精准的 bbox（0-1000的归一化坐标[xmin, ymin, xmax, ymax]）。\n",
    "\n",
    "    # 页面图像的MinerU布局检测结果：\n",
    "    # {layout_context}\n",
    "    \n",
    "    # 用户查询：'{user_query}'\n",
    "    \n",
    "    # 最终请输出相关证据列表，格式如下：\n",
    "    # ```json\n",
    "    # [\n",
    "    #   {{\n",
    "    #     \"evidence\": \"<self-contained evidence point, understandable without page context>\",\n",
    "    #     \"bbox\": [xmin, ymin, xmax, ymax],  # 0-1000归一化坐标\n",
    "    #     \"img_idx\": <the index of the zoomed-in image>\n",
    "    #     \"angle\": <int> \n",
    "    #   }},\n",
    "    #   ...\n",
    "    # ]\n",
    "    # ```\n",
    "    # \"\"\"\n",
    "    \n",
    "    prompt_text = f\"\"\"\n",
    "    The input includes:\n",
    "    1. The page image of a visual document.\n",
    "    2. Query-agnostic layout detection results (MinerU format). Note: This serves only as a reference and may contain noise such as incorrect granularity or inaccurate bounding boxes.\n",
    "    3. The user's query intent.\n",
    "\n",
    "    Please execute the following steps:\n",
    "    1. Semantic Matching: Carefully observe the image to determine if the page content contains evidence information relevant to the user's query. If it is irrelevant, return an empty list.\n",
    "    2. Precise Localization: If relevant, extract the complete chain of visual evidence that helps to answer the query (which may include elements such as multiple text blocks, tables, or image regions).\n",
    "    3. BBox Correction: If the MinerU bbox is too large (containing irrelevant content), too small (truncating content or over-split), or shifted, please generate a new, more precise bbox based on the actual visual content of the image (using 0-1000 normalized coordinates [xmin, ymin, xmax, ymax]).\n",
    "\n",
    "    MinerU layout detection for the page image:\n",
    "    {layout_context}\n",
    "\n",
    "    User Query: '{user_query}'\n",
    "\n",
    "    Finally, output the list of relevant evidence in the following format, return an empty list if not relevant:\n",
    "    ```json\n",
    "    [\n",
    "    {{\n",
    "        \"evidence\": \"<self-contained evidence point, understandable without page context>\",\n",
    "        \"bbox\": [xmin, ymin, xmax, ymax],  # 0-1000 normalized coordinates\n",
    "        \"img_idx\": <the index of the zoomed-in image>,\n",
    "        \"angle\": <int> \n",
    "    }},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "    \n",
    "    Let us think step by step!\n",
    "    \"\"\"\n",
    "\n",
    "    # 3. 构造消息体\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"image\": image_path},\n",
    "            {\"text\": prompt_text}\n",
    "        ]}\n",
    "    ]\n",
    "\n",
    "    last_response = \"\"\n",
    "    response_plain_text = ''\n",
    "    for ret_messages in agent.run(messages):\n",
    "        # `ret_messages` will contain all subsequent messages, consisting of interleaved assistant messages and tool responses\n",
    "        response_plain_text = multimodal_typewriter_print(ret_messages, response_plain_text)\n",
    "        last_response = ret_messages[-1]['content']\n",
    "\n",
    "    # 5. 解析输出 (清洗 Markdown 代码块标记)\n",
    "    try:\n",
    "        # 提取 last_response 中的文本内容\n",
    "        # 兼容 agent.run 返回的不同格式，如果是列表则取最后一条\n",
    "        full_text = last_response if isinstance(last_response, str) else last_response[0].get('text', '')\n",
    "\n",
    "        # 正则表达式说明：\n",
    "        # 使用 re.DOTALL 允许 . 匹配换行符\n",
    "        # 匹配 ```json [ ... ] ``` 或 ``` [ ... ] ``` 甚至直接的 [ ... ]\n",
    "        pattern = r\"```(?:json)?\\s*(\\[.*\\])\\s*```|(\\[.*\\])\"\n",
    "        match = re.search(pattern, full_text, re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            # group(1) 匹配带代码块的，group(2) 匹配不带代码块的\n",
    "            clean_json_str = match.group(1) if match.group(1) else match.group(2)\n",
    "            result_list = json.loads(clean_json_str.strip())\n",
    "        else:\n",
    "            # 如果正则没匹配到，尝试直接解析整个字符串（兜底）\n",
    "            result_list = json.loads(full_text.strip())\n",
    "        \n",
    "        result_list = json.loads(clean_json_str)\n",
    "        \n",
    "        # 简单验证返回格式\n",
    "        if isinstance(result_list, list):\n",
    "            return result_list\n",
    "        else:\n",
    "            print(\"Warning: Model return is not a list.\")\n",
    "            return []\n",
    "            \n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Failed to parse JSON response. Raw response: {last_response}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent initialized.\n"
     ]
    }
   ],
   "source": [
    "# LLM Configuration\n",
    "llm_cfg = {\n",
    "    'model_type': 'qwenvl_oai',\n",
    "    'model': 'qwen3-vl-plus',\n",
    "    # 'model': 'gemini-3-pro-preview',\n",
    "    # 'model_server': 'http://35.220.164.252:3888/v1', \n",
    "    # 'api_key': 'sk-ohsIxhcDUF0xwqqmFl1L1niRtEOD9LnvxFGjtjakXennNTzI', # REPLACE WITH YOUR ACTUAL KEY\n",
    "    'model_server': 'https://dashscope.aliyuncs.com/compatible-mode/v1',\n",
    "    'api_key': 'sk-1e374badf38a432c86886917fd8a867a',\n",
    "    'generate_cfg': {\n",
    "        'top_p': 0.8, \n",
    "        'top_k': 20,\n",
    "        'temperature': 1.0 # Lower temperature helps with stable JSON\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = ['image_zoom_in_tool', 'image_rotate_tool']\n",
    "\n",
    "# Initialize Agent\n",
    "# analysis_prompt = f\"\"\"Your role is that of a research assistant specializing in visual information. Answer questions about images by looking at them closely and then using research tools. Please follow this structured thinking process and show your work.\n",
    "\n",
    "# Start an iterative loop for each question:\n",
    "\n",
    "# - **First, look closely:** Begin with a detailed description of the image, paying attention to the user's question. List what you can tell just by looking, and what you'll need to look up.\n",
    "# - **Next, find information:** Use tools to research the things you need to find out. Think carefully before specifying the index of targeted image if there are multiple images.\n",
    "# - **Then, review the findings:** Carefully analyze what the tool tells you and decide on your next action.\n",
    "\n",
    "# Continue this loop until your research is complete.\n",
    "\n",
    "# To finish, bring everything together in a clear, synthesized answer that fully responds to the user's question.\"\"\"\n",
    "agent = Assistant(llm=llm_cfg, function_list=tools, system_message='')\n",
    "print(\"Agent initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 18:56:01,175 - base.py - 780 - INFO - ALL tokens: 1776, Available tokens: 58000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout data with 13 elements.\n",
      "Processing query: 'What is the brand of the machine in the image? '...\n",
      "[ANSWER]\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"evidence\": \"The image contains a group photograph of medical staff and a separate photo of Cmdr. Charmagne Beckett on the flight deck of the USNS Mercy (T-AH 19). There are no machines visible in either photo, only people and parts of the ship.\",\n",
      "        \"bbox\": [62, 46, 938, 956],\n",
      "        \"img_idx\": 0,\n",
      "        \"angle\": 0\n",
      "    }\n",
      "]\n",
      "```\n",
      "--- Extraction Results ---\n",
      "[\n",
      "  {\n",
      "    \"evidence\": \"The image contains a group photograph of medical staff and a separate photo of Cmdr. Charmagne Beckett on the flight deck of the USNS Mercy (T-AH 19). There are no machines visible in either photo, only people and parts of the ship.\",\n",
      "    \"bbox\": [\n",
      "      62,\n",
      "      46,\n",
      "      938,\n",
      "      956\n",
      "    ],\n",
      "    \"img_idx\": 0,\n",
      "    \"angle\": 0\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 1. Define paths (Update these paths to match your local environment)\n",
    "image_file = \"/root/LMUData/images/MMLongBench_DOC/0b85477387a9d0cc33fca0f4becaa0e5_4.jpg\"\n",
    "layout_file = \"/root/LMUData/parsed_results/MMLongBench_DOC/0b85477387a9d0cc33fca0f4becaa0e5_4.json\"\n",
    "\n",
    "# 2. Define Query\n",
    "# query = \"Who is the editor of NAVAL MEDICAL RESEARCH AND DEVELOPMENT\"\n",
    "query = \"What is the brand of the machine in the image? \"\n",
    "\n",
    "# 3. Load Layout Data\n",
    "try:\n",
    "    with open(layout_file, 'r', encoding='utf-8') as f:\n",
    "        layout_data = json.load(f)\n",
    "    print(f\"Loaded layout data with {len(layout_data)} elements.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Layout file not found at {layout_file}\")\n",
    "    # Create dummy data for testing if file is missing\n",
    "    layout_data = [] \n",
    "\n",
    "# 4. Execute Extraction\n",
    "if os.path.exists(image_file):\n",
    "    print(f\"Processing query: '{query}'...\")\n",
    "    results = extract_relevant_elements_with_vlm(image_file, layout_data, query, agent)\n",
    "    \n",
    "    print(\"\\n--- Extraction Results ---\")\n",
    "    print(json.dumps(results, indent=2, ensure_ascii=False))\n",
    "else:\n",
    "    print(f\"Error: Image file not found at {image_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
